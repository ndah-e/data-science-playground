{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "\n",
    "from uszipcode import SearchEngine\n",
    "from uszipcode import Zipcode\n",
    "\n",
    "import random\n",
    "import scipy.stats as stats\n",
    "\n",
    "import holidays\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "colors = mcolors.TABLEAU_COLORS\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data = pd.read_csv('data/station_data.csv')\n",
    "print(station_data.shape)\n",
    "print(station_data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(station_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import trip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data = pd.read_csv('data/trip_data.csv')\n",
    "print(trip_data.shape)\n",
    "print(trip_data.head())\n",
    "print(trip_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data = pd.read_csv('data/weather_data.csv')\n",
    "print(weather_data.shape)\n",
    "print(weather_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weather_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we link all three data sets to leverage all information contained in th dataset. We start by including zip codes in the station_data to be able to link to the weather data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add city names to the trips data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_city_name(x):\n",
    "    return station_data[station_data.Id == x].City.values[0]\n",
    "\n",
    "trip_data[\"Pick_up_city\"] = trip_data[\"Start Station\"].apply(lambda x: get_city_name(x))\n",
    "trip_data[\"Drop_off_city\"] = trip_data[\"End Station\"].apply(lambda x: get_city_name(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create time dependent variables and flag business days and holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data[\"Start_Date_time\"] =  pd.to_datetime(trip_data[\"Start Date\"], dayfirst=True)\n",
    "trip_data[\"End_Date_time\"] =  pd.to_datetime(trip_data[\"End Date\"], dayfirst=True)\n",
    "\n",
    "trip_data[\"Start_Date\"] =  trip_data[\"Start_Date_time\"].dt.date\n",
    "trip_data[\"End_Date\"] =  trip_data[\"End_Date_time\"].dt.date\n",
    "\n",
    "trip_data['Start_month'] = trip_data['Start_Date_time'].dt.month\n",
    "trip_data['Start_weekday'] = trip_data['Start_Date_time'].dt.dayofweek\n",
    "trip_data['Start_hour'] = trip_data['Start_Date_time'].dt.hour\n",
    "\n",
    "trip_data['End_month'] = trip_data['End_Date_time'].dt.month\n",
    "trip_data['End_weekday'] = trip_data['End_Date_time'].dt.dayofweek\n",
    "trip_data['End_hour'] = trip_data['End_Date_time'].dt.hour\n",
    "\n",
    "trip_data['is_weekend_start'] = trip_data.Start_weekday.isin([5,6])*1\n",
    "trip_data['is_weekend_end'] = trip_data.End_weekday.isin([5,6])*1\n",
    "\n",
    "# Reset date time to top of the hour\n",
    "trip_data[\"Start_Date_hour\"] = trip_data['Start_Date_time'].values.astype('<M8[h]')\n",
    "trip_data[\"End_Date_hour\"] = trip_data['End_Date_time'].values.astype('<M8[h]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "holidays =  holidays.US(state='CA', years=[2014,2015])\n",
    "\n",
    "weekDays = {0:\"Monday\", 1:\"Tuesday\", 2:\"Wednesday\", 3:\"Thursday\", 4:\"Friday\", 5:\"Saturday\", 6:\"Sunday\"}\n",
    "\n",
    "def set_holidays_to_7(x, holidays=holidays):\n",
    "    if x in holidays:\n",
    "        return 7\n",
    "    else:\n",
    "        return x.weekday()\n",
    "            \n",
    "def set_names(x, holidays=holidays):\n",
    "    return weekDays[x.weekday()]\n",
    "\n",
    "def check_holidays(x, holidays=holidays):\n",
    "    if x in holidays:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def check_business_day(x, holidays=holidays):\n",
    "    if x in holidays or x.weekday() in [5,6]:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "trip_data[\"holiday\"] = trip_data[\"Start_Date\"].apply(lambda x: check_holidays(x))\n",
    "trip_data[\"Business_day\"] = trip_data[\"Start_Date\"].apply(lambda x: check_business_day(x))\n",
    "trip_data[\"Start_weekday_no\"] = trip_data[\"Start_Date\"].apply(lambda x: set_holidays_to_7(x))\n",
    "trip_data[\"Start_day_name\"] = trip_data[\"Start_Date\"].apply(lambda x: set_names(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore trip per city and stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = station_data['City'].values\n",
    "unique, counts = np.unique(classes, return_counts=True)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(unique,counts, color=colors)\n",
    "plt.title('Number of stations per city')\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Number of stations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the number of trips on each month.\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "classes = trip_data['Subscriber Type'].values\n",
    "unique, counts = np.unique(classes, return_counts=True)\n",
    "axes[0].bar(unique,counts, color=colors)\n",
    "axes[0].set_xlabel('Subscriber Type')\n",
    "axes[0].set_ylabel('Number of trips')\n",
    "axes[0].set_title('Subscriber Type Frequency')\n",
    "\n",
    "classes = station_data['City'].values\n",
    "unique, counts = np.unique(classes, return_counts=True)\n",
    "axes[1].bar(unique,counts, color=colors)\n",
    "axes[1].set_xlabel('City')\n",
    "axes[1].set_ylabel('Number of trips')\n",
    "axes[1].set_title('Number of stations per city')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_out = trip_data.loc[trip_data['Pick_up_city']!= trip_data['Drop_off_city']]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "classes = trip_data_out['Pick_up_city'].values\n",
    "unique, counts = np.unique(classes, return_counts=True)\n",
    "axes[0].bar(unique,counts, color=colors)\n",
    "axes[0].set_xlabel('City')\n",
    "axes[0].set_ylabel('Number of trips')\n",
    "axes[0].set_title('Out of city Start Station')\n",
    "\n",
    "classes = trip_data_out['Drop_off_city'].values\n",
    "unique, counts = np.unique(classes, return_counts=True)\n",
    "axes[1].bar(unique,counts, color=colors)\n",
    "axes[1].set_xlabel('City')\n",
    "axes[1].set_ylabel('Number of trips')\n",
    "axes[1].set_title('Out of city End Station')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in station_data.City.unique():\n",
    "    total_trips = trip_data[trip_data.Pick_up_city == city].shape[0]\n",
    "    num_out_of_city = trip_data_out[trip_data_out.Pick_up_city == city].shape[0]\n",
    "    ratio = num_out_of_city/total_trips\n",
    "    print('The percentage of trips from a start station in {} to a different city is {:.3f}% out of total {} trips.'.format\n",
    "          (city, ratio, total_trips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = trip_data_out['Subscriber Type'].values\n",
    "unique, counts = np.unique(classes, return_counts=True)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(unique,counts, color=colors)\n",
    "plt.title('Subscriber Type Frequency')\n",
    "plt.xlabel('Subscriber Type')\n",
    "plt.ylabel('Number of trips')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = trip_data['Start Station'].values\n",
    "unique_start, counts_start = np.unique(classes, return_counts=True)\n",
    "classes = trip_data['End Station'].values\n",
    "unique_end, counts_end = np.unique(classes, return_counts=True)\n",
    "\n",
    "plt.figure(figsize=(24,10))\n",
    "plt.title('Pict-up/drop-off frequency per station', fontsize=18)\n",
    "plt.scatter(unique_start,counts_start,label='Pick up')\n",
    "plt.scatter(unique_end,counts_end, label='Drop off')\n",
    "\n",
    "plt.xlabel('Station ID', fontsize=20)\n",
    "plt.ylabel('Counts', fontsize=20)\n",
    "\n",
    "plt.tick_params(labelsize=11)\n",
    "plt.xticks(unique_start)\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in station_data.iterrows():\n",
    "    start_trips = len(trip_data[trip_data['Start Station'] == row.Id])\n",
    "    end_trips = len(trip_data[trip_data['End Station'] == row.Id])\n",
    "    station_data.at[i, \"net_rate\"] = end_trips - start_trips\n",
    "    \n",
    "fig = plt.figure(figsize=(20,10))\n",
    "fig.suptitle('Net rate per City', fontsize=20)\n",
    "\n",
    "i = 231\n",
    "for city in set(station_data.City):\n",
    "    tmp_df = station_data[station_data.City == city]\n",
    "    pos_signal = tmp_df.net_rate.copy()\n",
    "    neg_signal = tmp_df.net_rate.copy()\n",
    "    pos_signal[pos_signal <= 0] = np.nan\n",
    "    neg_signal[neg_signal > 0] = np.nan\n",
    "    \n",
    "    x_pos = [i for i, _ in enumerate(tmp_df.Id)]\n",
    "    \n",
    "    ax = fig.add_subplot(i)\n",
    "    ax.set_title(city)\n",
    "    ax.bar(x_pos, pos_signal, color='b', align='center')\n",
    "    ax.bar(x_pos, neg_signal, color='r', align='center')\n",
    "    plt.xticks(x_pos, tmp_df.Id)\n",
    "    \n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With above net rate distribution shows that the pick up and drop off varies across stations. Some stations seem to get more bike dropped off than they are picked up indicating that the location of the station might be important in predicting the net rental rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore trips by days and hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are a total of {} business days in the time period under investigation.'.format\n",
    "      (len(trip_data[trip_data['Business_day']==True].Start_Date.unique())))\n",
    "print('There are a total of {} days in the time period under investigation.'.format(len(trip_data['Start_Date'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trips per weekday and holidays\n",
    "weekDays = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\", 'Holiday']\n",
    "val = [0,1,2,3,4,5,6,7]\n",
    "plt.subplots(figsize=(10, 6))\n",
    "trip_data.groupby('Start_month')['Trip ID'].count().plot('bar')\n",
    "plt.xlabel(\"Months\")\n",
    "plt.ylabel(\"Number of trips\")\n",
    "plt.xticks(rotation='horizontal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekDays = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\", 'Holiday']\n",
    "val = [0,1,2,3,4,5,6,7]\n",
    "plt.subplots(figsize=(10, 6))\n",
    "trip_data.groupby('Start_weekday_no')['Trip ID'].count().plot('bar')\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Number of trips\")\n",
    "plt.xticks(val, weekDays, rotation='horizontal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 6))\n",
    "trip_data.groupby('Start_hour')['Trip ID'].count().plot('bar')\n",
    "plt.title(\"Start hour Distribution \")\n",
    "plt.xticks(rotation='horizontal', fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel(\"Hours\", fontsize=18)\n",
    "plt.ylabel(\"Frequency\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(10, 6))\n",
    "trip_data.groupby('End_hour')['Trip ID'].count().plot('bar')\n",
    "plt.title(\"End hour Distribution \")\n",
    "plt.xticks(rotation='horizontal', fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel(\"Hours\", fontsize=18)\n",
    "plt.ylabel(\"Frequency\", fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_freq_by_subscription(var):\n",
    "    \n",
    "    weekDays = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    fig.subplots_adjust(wspace=0.3, hspace=0.6)\n",
    "    fig.tight_layout()\n",
    "    title = 'Trip frequency by ' + var + ' per City'\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "\n",
    "    i = 1\n",
    "    for city in set(station_data.City):\n",
    "        tmp_trip = trip_data[trip_data.Pick_up_city == city]\n",
    "        ax = fig.add_subplot(5, 3, i)\n",
    "        tmp_trip.groupby(var)['Trip ID'].count().plot('bar')\n",
    "        ax.set_title(city)\n",
    "        i = i+1\n",
    "        for subs in set(trip_data['Subscriber Type']):\n",
    "            tmp_trip = trip_data.loc[(trip_data['Pick_up_city'] == city) & (trip_data['Subscriber Type'] == subs)]\n",
    "            ax = fig.add_subplot(5, 3, i)\n",
    "            if 'Start_day' in var:\n",
    "                tmp_trip.set_index(var).loc[weekDays].groupby(var)['Trip ID'].plot('bar')\n",
    "            else:\n",
    "                tmp_trip.groupby(var)['Trip ID'].count().plot('bar')\n",
    "            \n",
    "            title2 = city + \" (\" + subs + \")\"\n",
    "            ax.set_title(title2)\n",
    "            i = i + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_freq_by_subscription('Start_hour') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 7 is attributed to all holidays\n",
    "plot_freq_by_subscription('Start_weekday_no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_freq_by_subscription('Start_month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the rental frequency over time across all stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_trips = trip_data.groupby('Start_Date').agg({'Business_day': ['median', 'count']})\n",
    "\n",
    "BDay_trips = daily_trips[daily_trips['Business_day']['median'] == True].reset_index()\n",
    "nonBDay_trips = daily_trips[daily_trips['Business_day']['median'] == False].reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "plt.plot(BDay_trips.Start_Date, BDay_trips['Business_day']['count'], 'bo', label='Business Day')\n",
    "plt.plot(nonBDay_trips.Start_Date, nonBDay_trips['Business_day']['count'], 'ro', label='Non-business Day')\n",
    "\n",
    "months = mdates.MonthLocator()\n",
    "year_month_Fmt = mdates.DateFormatter('%y/%m')\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(year_month_Fmt)\n",
    "\n",
    "plt.xticks(rotation='horizontal', fontsize=16)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xlabel(\"Days\", fontsize=18)\n",
    "plt.legend(fontsize=18)\n",
    "plt.ylabel('Trip Counts', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rental duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data['Trip_duration'] = trip_data[\"End_Date_time\"] - trip_data[\"Start_Date_time\"]\n",
    "trip_data['Trip_duration']=trip_data['Trip_duration']/np.timedelta64(1,'h')\n",
    "#trip_data['Trip_duration']=trip_data['Trip_duration']/np.timedelta64(1,'m')\n",
    "\n",
    "print(trip_data['Trip_duration'].describe())\n",
    "print()\n",
    "print('Average ride time (hours) {} .'.format(trip_data['Trip_duration'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "sns.distplot(trip_data['Trip_duration'], hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that most of the trips are under 24hrs with an average duration of about 20mins. we can define a rudimentary threshold to eliminate possible outling trips using the simple mead + 2*std as the cutoff and all trips about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window = trip_data['Trip_duration'].mean() + 2*trip_data['Trip_duration'].std()\n",
    "print('Threshold of outlying trips (hours) {} .'.format(window))\n",
    "\n",
    "trip_data_out = trip_data[trip_data['Trip_duration'] <= window]\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "x = trip_data_out.Trip_duration\n",
    "sns.distplot(x, hist=False, kde=True, \n",
    "             bins=int(180/5), color = 'darkblue', \n",
    "             hist_kws={'edgecolor':'black'},\n",
    "             kde_kws={'linewidth': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_duration_subscriber = trip_data[trip_data['Subscriber Type']=='Subscriber'].Trip_duration.mean()\n",
    "trip_duration_customer = trip_data[trip_data['Subscriber Type']=='Customer'].Trip_duration.mean()\n",
    "\n",
    "print('The average trip duration for a subscriber is %.1f hours.' % trip_duration_subscriber)\n",
    "print('The average trip duration for a non-subscriber is %.1f hours.' % trip_duration_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in station_data.City.unique():\n",
    "    duration = trip_data[trip_data['Pick_up_city']==city].Trip_duration.mean()\n",
    "    print('The average trip duration for {} is {:.2f}. hours'.format(city, duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate effect of station change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_changed_station = {}\n",
    "stations = [23, 25, 49, 69, 72, 85, 86, 87, 88, 89, 90]\n",
    "hours = set(trip_data.Start_hour)\n",
    "\n",
    "station_list = []\n",
    "hour_list = []\n",
    "trip_start_list = []\n",
    "trip_end_list = []\n",
    "net_rate_list = []\n",
    "\n",
    "for station in stations:\n",
    "    for hour in range(24):\n",
    "        trip_start = len(trip_data[(trip_data['Start Station'] == station) & (trip_data['Start_hour'] == hour)])\n",
    "        trip_end = len(trip_data[(trip_data['End Station'] == station) & (trip_data['End_hour'] == hour)])\n",
    "        net_rate = trip_end - trip_start\n",
    "        \n",
    "        trip_start_list.append(trip_start)\n",
    "        trip_end_list.append(trip_end)\n",
    "        station_list.append(station)\n",
    "        hour_list.append(hour)\n",
    "        net_rate_list.append(net_rate)\n",
    "        \n",
    "# intialise data of lists. \n",
    "data = {'Start_station':station_list, 'hour':hour_list, 'trip_starts': trip_start_list, \n",
    "        'trip_ends':trip_end_list,'net_rate': net_rate_list} \n",
    "df = pd.DataFrame(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.stats import wilcoxon\n",
    "station_change = {\n",
    "    23: 85,\n",
    "    25: 86,\n",
    "    49: 87,\n",
    "    69: 88,\n",
    "    72: 89,\n",
    "    89: 90,\n",
    "    90: 72\n",
    "}\n",
    "\n",
    "for station1, station2 in station_change.items(): \n",
    "    print(\"Comparison between \", str(station1), \" vs \", str(station2))\n",
    "    group1 = df[df.Start_station == station1].net_rate.tolist()\n",
    "    group2 = df[df.Start_station == station2].net_rate.tolist()\n",
    "    u_statistic, pVal = wilcoxon(group1, group2)\n",
    "    print ('P value: ' + str(pVal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_change = {\n",
    "    23: 85,\n",
    "    25: 86,\n",
    "    49: 87,\n",
    "    69: 88,\n",
    "    72: 89,\n",
    "    89: 90\n",
    "}\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.6)\n",
    "fig.tight_layout()\n",
    "\n",
    "i = 1\n",
    "for station1, station2 in station_change.items():\n",
    "    \n",
    "    hour = df[df.Start_station == station1].hour.tolist()\n",
    "    group1 = df[df.Start_station == station1].net_rate.tolist()\n",
    "    group2 = df[df.Start_station == station2].net_rate.tolist()\n",
    "    \n",
    "    ax = fig.add_subplot(3, 2, i)\n",
    "    title = 'Net rate for ' + str(station1) + ' vs ' + str(station2)\n",
    "    plt.title(title)\n",
    "    plt.plot(hour, group1,label=station1, color='r')\n",
    "    plt.plot(hour, group2,label=station2, color='b')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Net rate')\n",
    "    plt.xticks(hour)\n",
    "    plt.legend()\n",
    "    i = i + 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,15))\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.6)\n",
    "fig.tight_layout()\n",
    "\n",
    "i = 1\n",
    "for station1, station2 in station_change.items():\n",
    "    \n",
    "    hour = df[df.Start_station == station1].hour.tolist()\n",
    "    group1 = df[df.Start_station == station1].trip_ends.tolist()\n",
    "    group2 = df[df.Start_station == station2].trip_ends.tolist()\n",
    "    \n",
    "    ax = fig.add_subplot(3, 2, i)\n",
    "    title = 'Trip end count for ' + str(station1) + ' vs ' + str(station2)\n",
    "    plt.title(title)\n",
    "    plt.plot(hour, group1,label=station1, color='r')\n",
    "    plt.plot(hour, group2,label=station2, color='b')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Net rate')\n",
    "    plt.xticks(hour)\n",
    "    plt.legend()\n",
    "\n",
    "    i = i + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There in no strong statistically significant evidence that the move station have impact on the net rate distribution. Bas on this we update the station in the trip data in order to have a complete time series flow for all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### update station info for moved stations\n",
    "station_update = {\n",
    "    23: 85,\n",
    "    25: 86,\n",
    "    49: 87,\n",
    "    69: 88,\n",
    "    89: 90,\n",
    "    72: 90\n",
    "}\n",
    "\n",
    "def value_to_update(x):\n",
    "    if x in station_update:\n",
    "        return station_update[x]\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "trip_data[\"Start Station\"] = trip_data[\"Start Station\"].apply(lambda x: value_to_update(x))\n",
    "trip_data[\"End Station\"] = trip_data[\"End Station\"].apply(lambda x: value_to_update(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data[\"Date_time\"] =  pd.to_datetime(weather_data[\"Date\"], dayfirst=True)\n",
    "weather_data['Date_str'] = weather_data.Date_time.astype(str)\n",
    "weather_data['Month'] = weather_data['Date_time'].dt.month\n",
    "weather_data['Weekday'] = weather_data['Date_time'].dt.dayofweek\n",
    "weather_data[\"Date_time\"] =  pd.to_datetime(weather_data[\"Date\"], dayfirst=True).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in weather_data.columns:\n",
    "    if pd.api.types.is_numeric_dtype(weather_data[col]):\n",
    "        weather_data[col].fillna(weather_data.groupby(\"Zip\")[col].transform(\"mean\"), inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We assume if there is an nan in the event then it is most likely a day without rain fog or Thunderstorm\n",
    "print(weather_data.Events.unique())\n",
    "weather_data['Events'].fillna('Clear', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume non gust values imply there was no wind gust and thus we set it to zero\n",
    "weather_data.loc[weather_data['Max Gust SpeedMPH'] == 0, 'Max Gust SpeedMPH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data['Max Gust SpeedMPH'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a correlation table between all the features.\n",
    "weather_corr_table = weather_data.corr(method='pearson', min_periods=1)\n",
    "display(weather_corr_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cols = ['Max TemperatureF', 'Mean TemperatureF', 'Min TemperatureF','Max Dew PointF', 'MeanDew PointF', 'Min DewpointF', \n",
    "             'Max Humidity','Mean Humidity', 'Min Humidity', 'Max Sea Level PressureIn','Mean Sea Level PressureIn', \n",
    "             'Min Sea Level PressureIn','Max VisibilityMiles', 'Mean VisibilityMiles', 'Min VisibilityMiles',\n",
    "             'Max Wind SpeedMPH', 'Mean Wind SpeedMPH', 'Max Gust SpeedMPH','PrecipitationIn', 'CloudCover', 'WindDirDegrees']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "corr = weather_data[corr_cols].corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = weather_data.corr(method='pearson', min_periods=1)\n",
    "r_squared = 0.5\n",
    "\n",
    "corr = []\n",
    "for col in corr_matrix.columns:\n",
    "    feature = corr_matrix[col]\n",
    "    corr_cols = feature[(feature.pow(2) > r_squared) & (feature != 1)]\n",
    "    corr_idx = corr_cols.index.values.tolist()\n",
    "    corr_dict = {col: corr_idx}\n",
    "    if corr_idx != []:\n",
    "        corr.append(corr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove highly correlated colunns\n",
    "selected_cols = ['Date', 'Mean TemperatureF', 'Mean Humidity', 'Mean Sea Level PressureIn','Max VisibilityMiles', \n",
    "                 'Min VisibilityMiles','Max Wind SpeedMPH', 'Max Gust SpeedMPH','PrecipitationIn', 'CloudCover', \n",
    "                 'Events', 'WindDirDegrees', 'Zip','Date_time', 'Date_str', 'Month', 'Weekday']\n",
    "#weather_data = weather_data[selected_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected there is a very high correlation between the min, mean and max values for each weather measurement.  \n",
    "Also there is a significant correlation between temperature and Due point measures  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_temp = pd.DataFrame(weather_data.groupby('Date')['Mean TemperatureF'].mean())\n",
    "daily_temp.index = pd.to_datetime(daily_temp.index, dayfirst=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "plt.plot(daily_temp.index, daily_temp['Mean TemperatureF'], 'bo')\n",
    "months = mdates.MonthLocator()\n",
    "year_month_Fmt = mdates.DateFormatter('%y/%m')\n",
    "ax.xaxis.set_major_locator(months)\n",
    "ax.xaxis.set_major_formatter(year_month_Fmt)\n",
    "plt.xticks(rotation='horizontal')\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel('Temperature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate the trips over each out to calculate net rate per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_time_num(x):\n",
    "    if x.hour in [6,7,8,9]:\n",
    "        return datetime.datetime(x.year, x.month, x.day, 1, 0, 0)\n",
    "    elif x.hour in [10,11,12,13,14]:\n",
    "        return datetime.datetime(x.year, x.month, x.day, 2, 0, 0)\n",
    "    elif x.hour in [15,16,17,18,19]:\n",
    "        return datetime.datetime(x.year, x.month, x.day, 3, 0, 0)\n",
    "    else:\n",
    "        return datetime.datetime(x.year, x.month, x.day, 4, 0, 0)\n",
    "\n",
    "def bin_time(x):\n",
    "    if x.hour in [6,7,8,9,15,16,17,18,19]:\n",
    "        return \"peak\"\n",
    "    elif x.hour in [10,11,12,13,14]:\n",
    "        return \"low peak\"\n",
    "    else:\n",
    "        return \"off peak\"\n",
    "\n",
    "trip_data[\"Start_Date_time_bin\"] = trip_data[\"Start_Date_time\"].apply(lambda x: bin_time_num(x))\n",
    "trip_data[\"End_Date_time_bin\"] = trip_data[\"End_Date_time\"].apply(lambda x: bin_time_num(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop outlining trips\n",
    "trip_data = trip_data[trip_data['Trip_duration'] <= window]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def binary_day(x, holidays=holidays):\n",
    "    x = pd.to_datetime(x)\n",
    "    holiday = 0\n",
    "    bday = 1\n",
    "    weekday = 1\n",
    "    \n",
    "    if x in holidays:\n",
    "        holiday = 1\n",
    "    \n",
    "    if x in holidays or x.weekday in [5,6]:\n",
    "        bday = 0\n",
    "        \n",
    "    if x.weekday in [5,6]:\n",
    "        weekday = 0\n",
    "    \n",
    "    return bday, holiday, weekday\n",
    "\n",
    "    \n",
    "def month_day_hour(x):\n",
    "    #x = x.to_datetime(x)\n",
    "    return x.month, x.weekday(), x.hour\n",
    "\n",
    "\n",
    "def aggregate_station_level(trip_df, station_df, weather_df, start_time, end_time):\n",
    "    \n",
    "    zipcode = {'Redwood City' : 94063,'San Francisco': 94107,'Palo Alto': 94301,'Mountain View': 94041,'San Jose': 95113}\n",
    "\n",
    "    date_list = []\n",
    "    station_list = []\n",
    "    city_list = []\n",
    "    hour_list = []\n",
    "    month_list = []\n",
    "    day_list = []\n",
    "    net_rate_list = []\n",
    "    holiday_list = []\n",
    "    Business_day_list = []\n",
    "    weekday_list = []\n",
    "    lat_list = []\n",
    "    long_list = []\n",
    "    zip_list = []\n",
    "    weather_list = []\n",
    "    \n",
    "    stations = station_df.Id.unique()\n",
    "    cities = station_df.City.unique()\n",
    "\n",
    "    for city in cities:\n",
    "        for station in stations:\n",
    "            \n",
    "            tmp_trip_start = trip_df[(trip_df.Pick_up_city == city) & (trip_df['Start Station'] == station)]\n",
    "            tmp_trip_end = trip_df[(trip_df.Pick_up_city == city) & (trip_df['End Station'] == station)]\n",
    "            \n",
    "            dates = tmp_trip_start[start_time].append(tmp_trip_end[end_time])\n",
    "            dates = pd.to_datetime(dates.sort_values(ascending=True).unique())           \n",
    "            \n",
    "            station_zip = zipcode[station_data[station_data.Id == station].City.values[0]]\n",
    "        \n",
    "            for date in dates:\n",
    "                trip_start = tmp_trip_start[tmp_trip_start[start_time] == date]\n",
    "                trip_end = tmp_trip_end[tmp_trip_end[end_time] == date]\n",
    "                \n",
    "                date_to_str = str(date.date())\n",
    "                weather_of_day = weather_df[(weather_df['Date_str'].str.contains(date_to_str)) & \n",
    "                                              (weather_df.Zip == int(station_zip))]\n",
    "                try:\n",
    "                    weather_list.append(weather_of_day.values.tolist()[0])\n",
    "                except:\n",
    "                    weather_list.append([])\n",
    "                \n",
    "                zip_list.append(station_zip)\n",
    "                city_list.append(city)\n",
    "                station_list.append(station)\n",
    "                \n",
    "                lat_list.append(station_df[station_df.Id == station].Lat.tolist()[0])\n",
    "                long_list.append(station_df[station_df.Id == station].Long.tolist()[0])\n",
    "                date_list.append(pd.to_datetime(date))\n",
    "\n",
    "                bday, holiday, weekday = binary_day(date)\n",
    "                Business_day_list.append(bday)\n",
    "                holiday_list.append(holiday)\n",
    "                weekday_list.append(weekday)\n",
    "\n",
    "                net_rate = trip_end.shape[0] - trip_start.shape[0]\n",
    "                net_rate_list.append(net_rate)\n",
    "                \n",
    "                month, day, hour = month_day_hour(date)\n",
    "                month_list.append(month)\n",
    "                day_list.append(day)\n",
    "                hour_list.append(hour)\n",
    "    \n",
    "    \n",
    "    df = pd.DataFrame({'City':city_list,'Station':station_list,'Date':date_list,'Lat':lat_list, 'Long':long_list,\n",
    "                       'Business_day':Business_day_list, 'holiday':holiday_list, 'is_weekday':weekday_list, 'hour':hour_list,\n",
    "                       'month':month_list, 'day':day_list, 'net_rate':net_rate_list})\n",
    "    \n",
    "    df_w = pd.DataFrame(weather_list, columns = list(weather_df.columns))\n",
    "    trips = pd.concat([df, df_w], axis = 1)\n",
    "    \n",
    "    return trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_hour = aggregate_station_level(trip_data, station_data, weather_data, 'Start_Date_hour','End_Date_hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(trip_data_hour.net_rate, trip_data_hour['Mean TemperatureF'])\n",
    "plt.xlabel('Net rate')\n",
    "plt.ylabel('Mean Temperature')\n",
    "plt.title('Scatter plot Net rate vs Mean Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(trip_data_hour.net_rate, trip_data_hour['Mean Sea Level PressureIn'])\n",
    "plt.xlabel('Net rate')\n",
    "plt.ylabel('Mean Sea Level PressureIn')\n",
    "plt.title('Scatter plot Net rate vs Mean Sea Level PressureIn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,15))\n",
    "fig.subplots_adjust(wspace=0.3, hspace=0.6)\n",
    "fig.tight_layout()\n",
    "\n",
    "i = 1\n",
    "for city in station_data.City.unique():\n",
    "    \n",
    "    ax = fig.add_subplot(3, 2, i)\n",
    "    n, bins, patches = plt.hist(trip_data_hour[trip_data_hour.City == city].net_rate, 100, \n",
    "                                density=True, facecolor='g', alpha=0.75)\n",
    "    plt.xlabel('Net rate')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(city)\n",
    "\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The net rate distribution for the are similar in all towns except San Fracisco indicating that we might group these cities and develop a single prediction model for them and another for San Francisco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "cities = ['San Francisco', 'San Jose', 'Redwood City', 'Mountain View', 'Palo Alto']\n",
    "\n",
    "Francisco = trip_data_hour[trip_data_hour.City == 'San Francisco'].net_rate.tolist()\n",
    "Jose = trip_data_hour[trip_data_hour.City == 'San Jose'].net_rate.tolist()\n",
    "Redwood = trip_data_hour[trip_data_hour.City == 'Redwood City'].net_rate.tolist()\n",
    "Mountain = trip_data_hour[trip_data_hour.City == 'Mountain View'].net_rate.tolist()\n",
    "Palo = trip_data_hour[trip_data_hour.City == 'Palo Alto'].net_rate.tolist()\n",
    "u_statistic, pVal = kruskal(Francisco, Jose, Redwood, Mountain, Palo)\n",
    "print(\"P value of combined test \" + str(pVal) + \"\\n\")\n",
    "\n",
    "n = len(cities)\n",
    "for i in range(len(cities)):\n",
    "    city_i = trip_data_hour[trip_data_hour.City == cities[i]].net_rate.tolist()\n",
    "    for j in range(i+1,n):\n",
    "        city_j = trip_data_hour[trip_data_hour.City == cities[j]].net_rate.tolist()\n",
    "        print(\"Comparison between \" + cities[i] + \" vs \" + cities[j])\n",
    "        u_statistic, pVal = kruskal(city_i, city_j)\n",
    "        print ('P value: ' + str(pVal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression model\n",
    "\n",
    "First we create a moving average as a baseline model to evaluate our model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_hour = pd.read_csv('data/trip_data_hour_with_weather.csv')\n",
    "trip_data_hour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(series, n):\n",
    "    \"\"\"\n",
    "        Calculate average of last n observations\n",
    "    \"\"\"\n",
    "    return np.average(series[-n:])\n",
    "\n",
    "cities = trip_data_hour.City.unique()\n",
    "for city in cities:\n",
    "    ts = trip_data_hour[trip_data_hour.City == city]\n",
    "    ts = ts.set_index('Date')\n",
    "    avg = moving_average(ts.net_rate, 1)\n",
    "    print(\"Moving average for \" + city + \" is \" + str(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMovingAverage(series, window, city, plot_intervals=False, scale=2, plot_anomalies=False):\n",
    "\n",
    "    \"\"\"\n",
    "        series - dataframe with timeseries\n",
    "        window - rolling window size \n",
    "        plot_intervals - show confidence intervals\n",
    "        plot_anomalies - show anomalies \n",
    "    \"\"\"\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.title(\"Moving average\\n window size = {}\".format(window))\n",
    "    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n",
    "\n",
    "    # Plot confidence intervals for smoothed values\n",
    "    if plot_intervals:\n",
    "        rmse = root_mean_squared_error(series[window:], rolling_mean[window:])\n",
    "        deviation = np.std(series[window:] - rolling_mean[window:])\n",
    "        lower_bond = rolling_mean - (rmse + scale * deviation)\n",
    "        upper_bond = rolling_mean + (rmse + scale * deviation)\n",
    "        plt.plot(upper_bond, \"r--\", label=\"Upper Bond\")\n",
    "        plt.plot(lower_bond, \"r--\", label=\"Lower Bond\")\n",
    "        \n",
    "        # Having the intervals, find abnormal values\n",
    "        if plot_anomalies:\n",
    "            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n",
    "            anomalies[series<lower_bond] = series[series<lower_bond]\n",
    "            anomalies[series>upper_bond] = series[series>upper_bond]\n",
    "            plt.plot(anomalies, \"ro\", markersize=10)\n",
    "        \n",
    "    plt.plot(series[window:], label=\"Actual values\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = trip_data_hour.City.unique()\n",
    "cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving average for San Francisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#window = 4\n",
    "#cities = trip_data_hour.City.unique()\n",
    "city = 'San Jose'\n",
    "ts = trip_data_hour[trip_data_hour.City == city]\n",
    "stnt = ts.Station.unique()[0]\n",
    "ts = ts[ts.Station == stnt]\n",
    "ts = ts.set_index('Date')\n",
    "    \n",
    "for window in [2,4,8]:\n",
    "    plotMovingAverage(ts['2014-12-01':'2014-12-14'].net_rate, window, city, plot_intervals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use latitude and longitude to account for the different stations and drop the station ID.  \n",
    "We develop 2 models one for san Francisco and another for the other stations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from numpy import sort\n",
    "from xgboost import plot_importance\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding cyclic feature\n",
    "trip_data_hour['hour'] = np.sin(trip_data_hour.hour*(2.*np.pi/24))\n",
    "trip_data_hour['day'] = np.sin(trip_data_hour.day*(2.*np.pi/7))\n",
    "trip_data_hour['month'] = np.sin((trip_data_hour.month-1)*(2.*np.pi/12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_hour = pd.get_dummies(trip_data_hour, columns=[\"Events\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_city(x):\n",
    "    if x != 'San Francisco':\n",
    "        return \"Others\"\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "trip_data_hour[\"Region\"] = trip_data_hour[\"City\"].apply(lambda x: group_city(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "random_state = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['City', 'Station', 'Date','Zip', 'Date_time', 'Date_str', 'Month', 'Weekday', 'Region']\n",
    "\n",
    "numeric_cols =  ['Mean TemperatureF', 'Mean Humidity', 'Mean Sea Level PressureIn','Max Gust SpeedMPH','PrecipitationIn', \n",
    "                 'CloudCover', 'WindDirDegrees']\n",
    "\n",
    "#numeric_cols =  ['Mean TemperatureF', 'Mean Humidity', 'Mean Sea Level PressureIn','Max VisibilityMiles','Min VisibilityMiles',\n",
    "#                 'Max Wind SpeedMPH', 'Max Gust SpeedMPH','PrecipitationIn', 'CloudCover', 'WindDirDegrees']\n",
    "\n",
    "# 'Min VisibilityMiles', 'Max Wind SpeedMPH', 'Max VisibilityMiles'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### San Francisco Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Francisco = trip_data_hour[trip_data_hour.Region == 'San Francisco']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set aside the data for the last month (August 2015) as the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Francisco_test = Francisco[Francisco.Date_time >= '2015-08-01']\n",
    "Francisco = Francisco[Francisco.Date_time < '2015-08-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Francisco = trip_data_hour[trip_data_hour.Region == 'San Francisco']\n",
    "Francisco = Francisco.drop(cols_to_drop, axis=1)\n",
    "    \n",
    "X = Francisco.drop(['net_rate'], axis=1)\n",
    "y = Francisco.net_rate\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xtrain[numeric_cols])\n",
    "\n",
    "Xtrain[numeric_cols] = scaler.transform(Xtrain[numeric_cols])\n",
    "Xtest[numeric_cols] = scaler.transform(Xtest[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [GradientBoostingRegressor, RandomForestRegressor,Lasso, LinearRegression, Ridge, LinearSVR, SVR, XGBRegressor]\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xtrain, ytrain, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "rgs_dict = {}\n",
    "for rgs in regressors:\n",
    "    try:\n",
    "        rgs_function = rgs(random_state=random_state) \n",
    "    except:\n",
    "        rgs_function = rgs()\n",
    "        \n",
    "    model = rgs_function.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(root_mean_squared_error(y_pred, y_test))\n",
    "    rgs_dict[rgs.__name__] = rmse\n",
    "\n",
    "best_reg_model = min(rgs_dict, key = lambda x: rgs_dict.get(x))\n",
    "print('The best regressor is {} with root mean squared error of {:.1f}.'.format(best_reg_model, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(rgs_dict).sort_values(ascending=False).plot.barh(figsize=(8, 6), grid=True, fontsize=12)\n",
    "plt.xlabel('Root mean squared errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(random_state=random_state)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"RMSE: %.2f\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({'Varaible':X_train.columns,\n",
    "                                   'importance':model.feature_importances_}).sort_values(by='importance', ascending=False)\n",
    "plt.subplots(figsize=(6, 8))\n",
    "sns.barplot(x=\"importance\", y=\"Varaible\", data=feature_importance)\n",
    "plt.title('Features Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = sort(model.feature_importances_)\n",
    "thresholds = thresholds[::-1]\n",
    "for thresh in thresholds:\n",
    "    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "    select_X_train = selection.transform(X_train)\n",
    "    selection_model = GradientBoostingRegressor(random_state=random_state)\n",
    "    selection_model.fit(select_X_train, y_train)\n",
    "    select_X_test = selection.transform(X_test)\n",
    "    y_pred = selection_model.predict(select_X_test)\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    print(\"Thresh=%.3f, n=%d, RMSE: %.2f\" % (thresh, select_X_train.shape[1], rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feat = feature_importance.Varaible[:4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_params = {'learning_rate':[0.02, 0.05, 0.08], 'n_estimators':[150, 200, 250], 'min_samples_leaf':[3, 4, 5], \n",
    "              'max_depth':[8, 9, 10]}\n",
    "\n",
    "nfolds = ShuffleSplit(n_splits=10, test_size = 0.20, random_state=random_state)\n",
    "\n",
    "acc_scorer = make_scorer(root_mean_squared_error)\n",
    "gbr = GradientBoostingRegressor(random_state=random_state) \n",
    "grid = GridSearchCV(estimator = gbr, param_grid = gbr_params, scoring = acc_scorer, cv = nfolds, n_jobs=2, verbose=1)\n",
    "grid.fit(Xtrain[best_feat], ytrain.values)\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_model =  GradientBoostingRegressor(random_state=random_state)\n",
    "SF_model.fit(X[best_feat], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Francisco_test['net_rate_pred'] = SF_model.predict(Francisco_test[best_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(ts['2015-08-01':'2015-08-31'].net_rate_pred, \"g\", label=\"predicted\")\n",
    "plt.plot(ts['2015-08-01':'2015-08-31'].net_rate, \"b--\", label=\"observed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other regions Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['City', 'Station', 'Date','Zip', 'Date_time', 'Date_str', 'Month', 'Weekday', 'Region']\n",
    "\n",
    "Others = trip_data_hour[trip_data_hour.Region == 'Others']\n",
    "Others = Others.drop(cols_to_drop, axis=1)\n",
    "    \n",
    "X = Others.drop(['net_rate'], axis=1)\n",
    "y = Others.net_rate\n",
    "Xtrain_O, Xtest_O, ytrain_O, ytest_O = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(Xtrain_O[numeric_cols])\n",
    "\n",
    "Xtrain_O[numeric_cols] = scaler.transform(Xtrain_O[numeric_cols])\n",
    "Xtest_O[numeric_cols] = scaler.transform(Xtest_O[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_O.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [GradientBoostingRegressor, RandomForestRegressor,Lasso, LinearRegression, Ridge, LinearSVR, SVR, XGBRegressor]\n",
    "\n",
    "X_train_O, X_test_O, y_train_O, y_test_O = train_test_split(Xtrain_O, ytrain_O, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "rgs_dict_O = {}\n",
    "for rgs in regressors:\n",
    "    try:\n",
    "        rgs_function = rgs(random_state=random_state) \n",
    "    except:\n",
    "        rgs_function = rgs()\n",
    "        \n",
    "    model = rgs_function.fit(X_train_O, y_train_O.values)\n",
    "    y_pred_O = model.predict(X_test_O)\n",
    "    rmse_O = np.sqrt(root_mean_squared_error(y_pred_O, y_test_O))\n",
    "    rgs_dict_O[rgs.__name__] = rmse_O\n",
    "    \n",
    "best_reg_model_O = min(rgs_dict_O, key = lambda x: rgs_dict_O.get(x))\n",
    "print('The best regressor is {} with root mean squared error of {:.1f}.'.format(best_reg_model_O, rmse_O))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgs_dict_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(rgs_dict_O).sort_values(ascending=False).plot.barh(figsize=(8, 6), grid=True, fontsize=12)\n",
    "plt.xlabel('Root mean squared errors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sort\n",
    "from xgboost import plot_importance\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "model = GradientBoostingRegressor(random_state=random_state)\n",
    "X_train_O, X_test_O, y_train_O, y_test_O\n",
    "model.fit(X_train_O, y_train_O)\n",
    "y_pred = model.predict(X_test_O)\n",
    "rmse = root_mean_squared_error(y_test_O, y_pred)\n",
    "\n",
    "print(\"RMSE: %.2f\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({'Varaible':X_train_O.columns,\n",
    "                                   'importance':model.feature_importances_}).sort_values(by='importance', ascending=False)\n",
    "plt.subplots(figsize=(6, 8))\n",
    "sns.barplot(x=\"importance\", y=\"Varaible\", data=feature_importance.head(30))\n",
    "plt.title('Features Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = sort(model.feature_importances_)\n",
    "thresholds = thresholds[::-1]\n",
    "for thresh in thresholds:\n",
    "    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "    select_X_train = selection.transform(X_train_O)\n",
    "    selection_model = GradientBoostingRegressor(random_state=random_state)\n",
    "    selection_model.fit(select_X_train, y_train_O)\n",
    "    select_X_test = selection.transform(X_test_O)\n",
    "    y_pred = selection_model.predict(select_X_test)\n",
    "    rmse = root_mean_squared_error(y_test_O, y_pred)\n",
    "    print(\"Thresh=%.3f, n=%d, RMSE: %.2f\" % (thresh, select_X_train.shape[1], rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feat = feature_importance.feature[:4]\n",
    "best_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_params = {'learning_rate':[0.02, 0.05, 0.08], 'n_estimators':[150, 200, 250], 'min_samples_leaf':[3, 4, 5], \n",
    "              'max_depth':[8, 9, 10]}\n",
    "\n",
    "nfolds = ShuffleSplit(n_splits=10, test_size = 0.20, random_state=random_state)\n",
    "\n",
    "acc_scorer = make_scorer(root_mean_squared_error)\n",
    "gbr_others = GradientBoostingRegressor(random_state=random_state) \n",
    "grid = GridSearchCV(estimator = gbr_others, param_grid = gbr_params, scoring = acc_scorer, cv = nfolds, n_jobs=2, verbose=1)\n",
    "grid.fit(X_train_O[best_feat], y_train_O.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
