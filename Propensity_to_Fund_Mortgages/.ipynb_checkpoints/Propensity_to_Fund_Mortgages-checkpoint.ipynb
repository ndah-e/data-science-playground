{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/aarshayj/Analytics_Vidhya/blob/master/Articles/Parameter_Tuning_XGBoost_with_Example/XGBoost%20models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pandas.core.algorithms as algos\n",
    "from pandas import Series\n",
    "import scipy.stats.stats as stats\n",
    "import re\n",
    "import traceback\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/CAX_MortgageModeling_Train.csv\")\n",
    "test = pd.read_csv(\"data/CAX_MortgageModeling_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45642, 22)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>MORTGAGE NUMBER</th>\n",
       "      <th>PROPERTY VALUE</th>\n",
       "      <th>MORTGAGE PAYMENT</th>\n",
       "      <th>GDS</th>\n",
       "      <th>LTV</th>\n",
       "      <th>TDS</th>\n",
       "      <th>AMORTIZATION</th>\n",
       "      <th>MORTGAGE AMOUNT</th>\n",
       "      <th>RATE</th>\n",
       "      <th>...</th>\n",
       "      <th>PROPERTY TYPE</th>\n",
       "      <th>TERM</th>\n",
       "      <th>FSA</th>\n",
       "      <th>AGE RANGE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>INCOME TYPE</th>\n",
       "      <th>NAICS CODE</th>\n",
       "      <th>CREDIT SCORE</th>\n",
       "      <th>RESULT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CAX_Train_1</td>\n",
       "      <td>1</td>\n",
       "      <td>900000</td>\n",
       "      <td>5429</td>\n",
       "      <td>61.98</td>\n",
       "      <td>65.00</td>\n",
       "      <td>71.63</td>\n",
       "      <td>360</td>\n",
       "      <td>1040000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Single Detached</td>\n",
       "      <td>12</td>\n",
       "      <td>L4C</td>\n",
       "      <td>Under 25</td>\n",
       "      <td>Male</td>\n",
       "      <td>108000</td>\n",
       "      <td>8</td>\n",
       "      <td>44-45</td>\n",
       "      <td>681</td>\n",
       "      <td>FUNDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>CAX_Train_2</td>\n",
       "      <td>2</td>\n",
       "      <td>386000</td>\n",
       "      <td>2179</td>\n",
       "      <td>35.22</td>\n",
       "      <td>74.29</td>\n",
       "      <td>40.65</td>\n",
       "      <td>360</td>\n",
       "      <td>390000.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>Single Detached</td>\n",
       "      <td>12</td>\n",
       "      <td>L9T</td>\n",
       "      <td>70 and over</td>\n",
       "      <td>Male</td>\n",
       "      <td>78000</td>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>710</td>\n",
       "      <td>FUNDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>CAX_Train_3</td>\n",
       "      <td>3</td>\n",
       "      <td>531000</td>\n",
       "      <td>2152</td>\n",
       "      <td>30.97</td>\n",
       "      <td>80.00</td>\n",
       "      <td>35.41</td>\n",
       "      <td>360</td>\n",
       "      <td>424800.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>Single Detached</td>\n",
       "      <td>6</td>\n",
       "      <td>M1N</td>\n",
       "      <td>35-39</td>\n",
       "      <td>Female</td>\n",
       "      <td>87000</td>\n",
       "      <td>2</td>\n",
       "      <td>72</td>\n",
       "      <td>709</td>\n",
       "      <td>FUNDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>CAX_Train_4</td>\n",
       "      <td>4</td>\n",
       "      <td>1200000</td>\n",
       "      <td>5410</td>\n",
       "      <td>19.04</td>\n",
       "      <td>75.00</td>\n",
       "      <td>34.14</td>\n",
       "      <td>360</td>\n",
       "      <td>960000.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>...</td>\n",
       "      <td>Single Detached</td>\n",
       "      <td>12</td>\n",
       "      <td>M2M</td>\n",
       "      <td>45-49</td>\n",
       "      <td>Male</td>\n",
       "      <td>300000</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>761</td>\n",
       "      <td>FUNDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>CAX_Train_5</td>\n",
       "      <td>5</td>\n",
       "      <td>350000</td>\n",
       "      <td>3342</td>\n",
       "      <td>29.59</td>\n",
       "      <td>80.00</td>\n",
       "      <td>34.85</td>\n",
       "      <td>360</td>\n",
       "      <td>592000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Single Detached</td>\n",
       "      <td>12</td>\n",
       "      <td>L7G</td>\n",
       "      <td>50-54</td>\n",
       "      <td>Male</td>\n",
       "      <td>147000</td>\n",
       "      <td>8</td>\n",
       "      <td>62</td>\n",
       "      <td>762</td>\n",
       "      <td>FUNDED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unique_ID  MORTGAGE NUMBER  PROPERTY VALUE  MORTGAGE PAYMENT    GDS  \\\n",
       "0  CAX_Train_1                1          900000              5429  61.98   \n",
       "1  CAX_Train_2                2          386000              2179  35.22   \n",
       "2  CAX_Train_3                3          531000              2152  30.97   \n",
       "3  CAX_Train_4                4         1200000              5410  19.04   \n",
       "4  CAX_Train_5                5          350000              3342  29.59   \n",
       "\n",
       "     LTV    TDS  AMORTIZATION  MORTGAGE AMOUNT  RATE  ...    PROPERTY TYPE  \\\n",
       "0  65.00  71.63           360        1040000.0   4.0  ...  Single Detached   \n",
       "1  74.29  40.65           360         390000.0   4.5  ...  Single Detached   \n",
       "2  80.00  35.41           360         424800.0   3.5  ...  Single Detached   \n",
       "3  75.00  34.14           360         960000.0   5.5  ...  Single Detached   \n",
       "4  80.00  34.85           360         592000.0   5.0  ...  Single Detached   \n",
       "\n",
       "  TERM  FSA    AGE RANGE  GENDER  INCOME INCOME TYPE  NAICS CODE  \\\n",
       "0   12  L4C     Under 25    Male  108000           8       44-45   \n",
       "1   12  L9T  70 and over    Male   78000           2          56   \n",
       "2    6  M1N        35-39  Female   87000           2          72   \n",
       "3   12  M2M        45-49    Male  300000           8          54   \n",
       "4   12  L7G        50-54    Male  147000           8          62   \n",
       "\n",
       "   CREDIT SCORE  RESULT  \n",
       "0           681  FUNDED  \n",
       "1           710  FUNDED  \n",
       "2           709  FUNDED  \n",
       "3           761  FUNDED  \n",
       "4           762  FUNDED  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FUNDED        36023\n",
       "NOT FUNDED     9619\n",
       "Name: RESULT, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.RESULT.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45642 entries, 0 to 45641\n",
      "Data columns (total 22 columns):\n",
      "Unique_ID            45642 non-null object\n",
      "MORTGAGE NUMBER      45642 non-null int64\n",
      "PROPERTY VALUE       45642 non-null int64\n",
      "MORTGAGE PAYMENT     45642 non-null int64\n",
      "GDS                  45642 non-null float64\n",
      "LTV                  45642 non-null float64\n",
      "TDS                  45642 non-null float64\n",
      "AMORTIZATION         45642 non-null int64\n",
      "MORTGAGE AMOUNT      45642 non-null float64\n",
      "RATE                 45642 non-null float64\n",
      "MORTGAGE PURPOSE     45642 non-null object\n",
      "PAYMENT FREQUENCY    45642 non-null object\n",
      "PROPERTY TYPE        45642 non-null object\n",
      "TERM                 45642 non-null int64\n",
      "FSA                  45642 non-null object\n",
      "AGE RANGE            45642 non-null object\n",
      "GENDER               45642 non-null object\n",
      "INCOME               45642 non-null int64\n",
      "INCOME TYPE          45642 non-null int64\n",
      "NAICS CODE           45642 non-null object\n",
      "CREDIT SCORE         45642 non-null int64\n",
      "RESULT               45642 non-null object\n",
      "dtypes: float64(5), int64(8), object(9)\n",
      "memory usage: 7.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unique_ID            0.0\n",
       "MORTGAGE NUMBER      0.0\n",
       "PROPERTY VALUE       0.0\n",
       "MORTGAGE PAYMENT     0.0\n",
       "GDS                  0.0\n",
       "LTV                  0.0\n",
       "TDS                  0.0\n",
       "AMORTIZATION         0.0\n",
       "MORTGAGE AMOUNT      0.0\n",
       "RATE                 0.0\n",
       "MORTGAGE PURPOSE     0.0\n",
       "PAYMENT FREQUENCY    0.0\n",
       "PROPERTY TYPE        0.0\n",
       "TERM                 0.0\n",
       "FSA                  0.0\n",
       "AGE RANGE            0.0\n",
       "GENDER               0.0\n",
       "INCOME               0.0\n",
       "INCOME TYPE          0.0\n",
       "NAICS CODE           0.0\n",
       "CREDIT SCORE         0.0\n",
       "RESULT               0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_missing = train.isna()\n",
    "train_num_missing = train_missing.sum()\n",
    "train_num_missing / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('O'), dtype('int64'), dtype('float64')], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_types = pd.DataFrame(train.dtypes, columns=['type'])\n",
    "data_types['Feature'] = data_types.index\n",
    "data_types.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Unique_ID', 'RESULT', 'FSA']\n",
    "df_funded = train[train.RESULT == 'FUNDED']\n",
    "df_not_funded = train[train.RESULT == 'NOT FUNDED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_object(funded, not_funded, col):\n",
    "    fig= plt.figure(figsize=(12,5))\n",
    "    legend = ['Funded', 'Not Funded']\n",
    "    plt.hist([funded, not_funded], color=['red', 'blue'])\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend(legend)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_hist(funded, not_funded, col):\n",
    "    legend = ['Funded', 'Not Funded']\n",
    "    plt.hist([funded, not_funded], color=['red', 'blue'])\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend(legend)\n",
    "    plt.xticks(range(0, 7))\n",
    "    plt.yticks(range(1, 20))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i,row in data_types.iterrows():\n",
    "#    if row.Feature not in cols_to_drop:\n",
    "#        col = row.Feature\n",
    "#        funded = df_funded[col]\n",
    "#        not_funded = df_not_funded[col]\n",
    "#        if row.type == \"object\":\n",
    "#            plot_hist_object(funded, not_funded, col)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WOE and IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas.core.algorithms as algos\n",
    "from pandas import Series\n",
    "import scipy.stats.stats as stats\n",
    "import re\n",
    "import traceback\n",
    "import string\n",
    "\n",
    "max_bin = 20\n",
    "force_bin = 3\n",
    "\n",
    "# define a binning function\n",
    "def mono_bin(Y, X, n = max_bin):\n",
    "    \n",
    "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
    "    notmiss = df1[['X','Y']][df1.X.notnull()]\n",
    "    r = 0\n",
    "    while np.abs(r) < 1:\n",
    "        try:\n",
    "            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n",
    "            d2 = d1.groupby('Bucket', as_index=True)\n",
    "            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
    "            n = n - 1 \n",
    "        except Exception as e:\n",
    "            n = n - 1\n",
    "\n",
    "    if len(d2) == 1:\n",
    "        n = force_bin         \n",
    "        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n",
    "        if len(np.unique(bins)) == 2:\n",
    "            bins = np.insert(bins, 0, 1)\n",
    "            bins[1] = bins[1]-(bins[1]/2)\n",
    "        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n",
    "        d2 = d1.groupby('Bucket', as_index=True)\n",
    "    \n",
    "    d3 = pd.DataFrame({},index=[])\n",
    "    d3[\"MIN_VALUE\"] = d2.min().X\n",
    "    d3[\"MAX_VALUE\"] = d2.max().X\n",
    "    d3[\"COUNT\"] = d2.count().Y\n",
    "    d3[\"EVENT\"] = d2.sum().Y\n",
    "    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n",
    "    d3=d3.reset_index(drop=True)\n",
    "    \n",
    "    if len(justmiss.index) > 0:\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        d4[\"MAX_VALUE\"] = np.nan\n",
    "        d4[\"COUNT\"] = justmiss.count().Y\n",
    "        d4[\"EVENT\"] = justmiss.sum().Y\n",
    "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
    "        d3 = d3.append(d4,ignore_index=True)\n",
    "    \n",
    "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"VAR_NAME\"] = \"VAR\"\n",
    "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n",
    "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "    d3.IV = d3.IV.sum()\n",
    "    \n",
    "    return(d3)\n",
    "\n",
    "def char_bin(Y, X):\n",
    "        \n",
    "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
    "    notmiss = df1[['X','Y']][df1.X.notnull()]    \n",
    "    df2 = notmiss.groupby('X',as_index=True)\n",
    "    \n",
    "    d3 = pd.DataFrame({},index=[])\n",
    "    d3[\"COUNT\"] = df2.count().Y\n",
    "    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n",
    "    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
    "    d3[\"EVENT\"] = df2.sum().Y\n",
    "    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n",
    "    \n",
    "    if len(justmiss.index) > 0:\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        d4[\"MAX_VALUE\"] = np.nan\n",
    "        d4[\"COUNT\"] = justmiss.count().Y\n",
    "        d4[\"EVENT\"] = justmiss.sum().Y\n",
    "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
    "        d3 = d3.append(d4,ignore_index=True)\n",
    "    \n",
    "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"VAR_NAME\"] = \"VAR\"\n",
    "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n",
    "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "    d3.IV = d3.IV.sum()\n",
    "    d3 = d3.reset_index(drop=True)\n",
    "    \n",
    "    return(d3)\n",
    "\n",
    "def data_vars(df1, target):\n",
    "    \n",
    "    stack = traceback.extract_stack()\n",
    "    filename, lineno, function_name, code = stack[-2]\n",
    "    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n",
    "    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n",
    "    \n",
    "    x = df1.dtypes.index\n",
    "    count = -1\n",
    "    \n",
    "    for i in x:\n",
    "        if i.upper() not in (final.upper()):\n",
    "            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n",
    "                conv = mono_bin(target, df1[i])\n",
    "                conv[\"VAR_NAME\"] = i\n",
    "                count = count + 1\n",
    "            else:\n",
    "                conv = char_bin(target, df1[i])\n",
    "                conv[\"VAR_NAME\"] = i            \n",
    "                count = count + 1\n",
    "                \n",
    "            if count == 0:\n",
    "                iv_df = conv\n",
    "            else:\n",
    "                iv_df = iv_df.append(conv,ignore_index=True)\n",
    "    \n",
    "    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n",
    "    iv = iv.reset_index()\n",
    "    return(iv_df,iv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train\n",
    "df['target'] = df['RESULT'].apply(lambda x : 1 if x == 'FUNDED' else 0)  # Convert to numeric\n",
    "df = df.drop(['Unique_ID', 'RESULT', 'MORTGAGE NUMBER'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = train['target']\n",
    "X = df.drop(['target'],axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.10, random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_iv, IV = data_vars(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VAR_NAME</th>\n",
       "      <th>IV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>INCOME TYPE</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CREDIT SCORE</td>\n",
       "      <td>0.000322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GDS</td>\n",
       "      <td>0.000486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMORTIZATION</td>\n",
       "      <td>0.000701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TDS</td>\n",
       "      <td>0.002831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GENDER</td>\n",
       "      <td>0.003137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INCOME</td>\n",
       "      <td>0.003495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGE RANGE</td>\n",
       "      <td>0.003552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PAYMENT FREQUENCY</td>\n",
       "      <td>0.006673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TERM</td>\n",
       "      <td>0.008699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PROPERTY TYPE</td>\n",
       "      <td>0.016790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NAICS CODE</td>\n",
       "      <td>0.024748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MORTGAGE AMOUNT</td>\n",
       "      <td>0.035554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PROPERTY VALUE</td>\n",
       "      <td>0.040395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RATE</td>\n",
       "      <td>0.042052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LTV</td>\n",
       "      <td>0.046775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MORTGAGE PURPOSE</td>\n",
       "      <td>0.059908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MORTGAGE PAYMENT</td>\n",
       "      <td>0.073015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FSA</td>\n",
       "      <td>0.184769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             VAR_NAME        IV\n",
       "7         INCOME TYPE  0.000004\n",
       "2        CREDIT SCORE  0.000322\n",
       "4                 GDS  0.000486\n",
       "1        AMORTIZATION  0.000701\n",
       "17                TDS  0.002831\n",
       "5              GENDER  0.003137\n",
       "6              INCOME  0.003495\n",
       "0           AGE RANGE  0.003552\n",
       "13  PAYMENT FREQUENCY  0.006673\n",
       "18               TERM  0.008699\n",
       "14      PROPERTY TYPE  0.016790\n",
       "12         NAICS CODE  0.024748\n",
       "9     MORTGAGE AMOUNT  0.035554\n",
       "15     PROPERTY VALUE  0.040395\n",
       "16               RATE  0.042052\n",
       "8                 LTV  0.046775\n",
       "11   MORTGAGE PURPOSE  0.059908\n",
       "10   MORTGAGE PAYMENT  0.073015\n",
       "3                 FSA  0.184769"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IV.sort_values('IV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IV.to_csv('IV.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply WOE values to your dataframe columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_vars_list = x_train.columns\n",
    "transform_prefix = 'new_' # leave this value blank if you need replace the original column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PROPERTY VALUE', 'MORTGAGE PAYMENT', 'GDS', 'LTV', 'TDS',\n",
       "       'AMORTIZATION', 'MORTGAGE AMOUNT', 'RATE', 'MORTGAGE PURPOSE',\n",
       "       'PAYMENT FREQUENCY', 'PROPERTY TYPE', 'TERM', 'FSA', 'AGE RANGE',\n",
       "       'GENDER', 'INCOME', 'INCOME TYPE', 'NAICS CODE', 'CREDIT SCORE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform_vars_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_WOE(df, transform_vars_list, final_iv):\n",
    "    \n",
    "    for var in transform_vars_list:\n",
    "        small_df = final_iv[final_iv['VAR_NAME'] == var]\n",
    "        transform_dict = dict(zip(small_df.MAX_VALUE,small_df.WOE))\n",
    "        replace_cmd = ''\n",
    "        replace_cmd1 = ''\n",
    "        for i in sorted(transform_dict.items()):\n",
    "            replace_cmd = replace_cmd + str(i[1]) + str(' if x <= ') + str(i[0]) + ' else '\n",
    "            replace_cmd1 = replace_cmd1 + str(i[1]) + str(' if x == \"') + str(i[0]) + '\" else '\n",
    "        replace_cmd = replace_cmd + '0'\n",
    "        replace_cmd1 = replace_cmd1 + '0'\n",
    "        if replace_cmd != '0':\n",
    "            try:\n",
    "                df[transform_prefix + var] = df[var].apply(lambda x: eval(replace_cmd))\n",
    "            except:\n",
    "                df[transform_prefix + var] = df[var].apply(lambda x: eval(replace_cmd1))\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_new = apply_WOE(x_train, transform_vars_list, final_iv)\n",
    "#x_test_new = apply_WOE(x_test, transform_vars_list, final_iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in transform_vars_list:\n",
    "    small_df = final_iv[final_iv['VAR_NAME'] == var]\n",
    "    transform_dict = dict(zip(small_df.MAX_VALUE,small_df.WOE))\n",
    "    replace_cmd = ''\n",
    "    replace_cmd1 = ''\n",
    "    for i in sorted(transform_dict.items()):\n",
    "        replace_cmd = replace_cmd + str(i[1]) + str(' if x <= ') + str(i[0]) + ' else '\n",
    "        replace_cmd1 = replace_cmd1 + str(i[1]) + str(' if x == \"') + str(i[0]) + '\" else '\n",
    "    replace_cmd = replace_cmd + '0'\n",
    "    replace_cmd1 = replace_cmd1 + '0'\n",
    "    if replace_cmd != '0':\n",
    "        try:\n",
    "            x_train[transform_prefix + var] = x_train[var].apply(lambda x: eval(replace_cmd))\n",
    "        except:\n",
    "            x_train[transform_prefix + var] = x_train[var].apply(lambda x: eval(replace_cmd1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in transform_vars_list:\n",
    "    small_df = final_iv[final_iv['VAR_NAME'] == var]\n",
    "    transform_dict = dict(zip(small_df.MAX_VALUE,small_df.WOE))\n",
    "    replace_cmd = ''\n",
    "    replace_cmd1 = ''\n",
    "    for i in sorted(transform_dict.items()):\n",
    "        replace_cmd = replace_cmd + str(i[1]) + str(' if x <= ') + str(i[0]) + ' else '\n",
    "        replace_cmd1 = replace_cmd1 + str(i[1]) + str(' if x == \"') + str(i[0]) + '\" else '\n",
    "    replace_cmd = replace_cmd + '0'\n",
    "    replace_cmd1 = replace_cmd1 + '0'\n",
    "    if replace_cmd != '0':\n",
    "        try:\n",
    "            x_test[transform_prefix + var] = x_test[var].apply(lambda x: eval(replace_cmd))\n",
    "        except:\n",
    "            x_test[transform_prefix + var] = x_test[var].apply(lambda x: eval(replace_cmd1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import  metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, ytrain, dtest, ytest, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=ytrain.values)\n",
    "        xgtest = xgb.DMatrix(dtest[predictors].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], ytrain, eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(ytrain.values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(ytrain, dtrain_predprob))\n",
    "    \n",
    "    #Predict on testing data:\n",
    "    predprob = alg.predict_proba(dtest[predictors])[:,1]\n",
    "    print('AUC Score (Test): %f' % metrics.roc_auc_score(ytest, predprob))\n",
    "                \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1- Find the number of estimators for a high learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [x for x in train.columns if \"new\" in x]\n",
    "\n",
    "xgb1 = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27)\n",
    "\n",
    "modelfit(xgb1, x_train, y_train, x_test, y_test, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid seach on subsample and max_features\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test1 = {\n",
    "    'max_depth':range(3,10,2),\n",
    "    'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    "                                        min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    "                       param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid seach on subsample and max_features\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test2 = {\n",
    "    'max_depth':[4,5,6],\n",
    "    'min_child_weight':[4,5,6]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,\n",
    "                                        min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch2.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid seach on subsample and max_features\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test2b = {\n",
    "    'min_child_weight':[6,8,10,12]\n",
    "}\n",
    "gsearch2b = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=4,\n",
    "                                        min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test2b, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch2b.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch2b.grid_scores_, gsearch2b.best_params_, gsearch2b.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid seach on subsample and max_features\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test3 = {\n",
    "    'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=4,\n",
    "                                        min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch3.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2 = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=4,\n",
    "        min_child_weight=6,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27)\n",
    "modelfit(xgb2, x_train, y_train, x_test, y_test, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Grid seach on subsample and max_features\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test4 = {\n",
    "    'subsample':[i/10.0 for i in range(6,10)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n",
    "                                        min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch4.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tune subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid seach on subsample and max_features\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test5 = {\n",
    "    'subsample':[i/100.0 for i in range(75,90,5)],\n",
    "    'colsample_bytree':[i/100.0 for i in range(75,90,5)]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n",
    "                                        min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch5.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid seach on subsample and max_features\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test6 = {\n",
    "    'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch6 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n",
    "                                        min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test6, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch6.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch6.grid_scores_, gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid seach on subsample and max_features\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test7 = {\n",
    "    'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n",
    "}\n",
    "gsearch7 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n",
    "                                        min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test7, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch7.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch7.grid_scores_, gsearch7.best_params_, gsearch7.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb3 = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=4,\n",
    "        min_child_weight=6,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.005,\n",
    "        objective= 'binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27)\n",
    "modelfit(xgb3, x_train, y_train, x_test, y_test, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb4 = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=5000,\n",
    "        max_depth=4,\n",
    "        min_child_weight=6,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.005,\n",
    "        objective= 'binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27)\n",
    "modelfit(xgb4, x_train, y_train, x_test, y_test, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
