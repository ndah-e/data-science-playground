{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pandas.core.algorithms as algos\n",
    "from pandas import Series\n",
    "import scipy.stats.stats as stats\n",
    "import re\n",
    "import traceback\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/CAX_MortgageModeling_Train.csv\")\n",
    "test = pd.read_csv(\"data/CAX_MortgageModeling_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45642, 22)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>MORTGAGE NUMBER</th>\n",
       "      <th>PROPERTY VALUE</th>\n",
       "      <th>MORTGAGE PAYMENT</th>\n",
       "      <th>GDS</th>\n",
       "      <th>LTV</th>\n",
       "      <th>TDS</th>\n",
       "      <th>AMORTIZATION</th>\n",
       "      <th>MORTGAGE AMOUNT</th>\n",
       "      <th>RATE</th>\n",
       "      <th>...</th>\n",
       "      <th>PROPERTY TYPE</th>\n",
       "      <th>TERM</th>\n",
       "      <th>FSA</th>\n",
       "      <th>AGE RANGE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>INCOME TYPE</th>\n",
       "      <th>NAICS CODE</th>\n",
       "      <th>CREDIT SCORE</th>\n",
       "      <th>RESULT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CAX_Train_1</td>\n",
       "      <td>1</td>\n",
       "      <td>900000</td>\n",
       "      <td>5429</td>\n",
       "      <td>61.98</td>\n",
       "      <td>65.00</td>\n",
       "      <td>71.63</td>\n",
       "      <td>360</td>\n",
       "      <td>1040000.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Single Detached</td>\n",
       "      <td>12</td>\n",
       "      <td>L4C</td>\n",
       "      <td>Under 25</td>\n",
       "      <td>Male</td>\n",
       "      <td>108000</td>\n",
       "      <td>8</td>\n",
       "      <td>44-45</td>\n",
       "      <td>681</td>\n",
       "      <td>FUNDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>CAX_Train_2</td>\n",
       "      <td>2</td>\n",
       "      <td>386000</td>\n",
       "      <td>2179</td>\n",
       "      <td>35.22</td>\n",
       "      <td>74.29</td>\n",
       "      <td>40.65</td>\n",
       "      <td>360</td>\n",
       "      <td>390000.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>Single Detached</td>\n",
       "      <td>12</td>\n",
       "      <td>L9T</td>\n",
       "      <td>70 and over</td>\n",
       "      <td>Male</td>\n",
       "      <td>78000</td>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>710</td>\n",
       "      <td>FUNDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>CAX_Train_3</td>\n",
       "      <td>3</td>\n",
       "      <td>531000</td>\n",
       "      <td>2152</td>\n",
       "      <td>30.97</td>\n",
       "      <td>80.00</td>\n",
       "      <td>35.41</td>\n",
       "      <td>360</td>\n",
       "      <td>424800.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>Single Detached</td>\n",
       "      <td>6</td>\n",
       "      <td>M1N</td>\n",
       "      <td>35-39</td>\n",
       "      <td>Female</td>\n",
       "      <td>87000</td>\n",
       "      <td>2</td>\n",
       "      <td>72</td>\n",
       "      <td>709</td>\n",
       "      <td>FUNDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>CAX_Train_4</td>\n",
       "      <td>4</td>\n",
       "      <td>1200000</td>\n",
       "      <td>5410</td>\n",
       "      <td>19.04</td>\n",
       "      <td>75.00</td>\n",
       "      <td>34.14</td>\n",
       "      <td>360</td>\n",
       "      <td>960000.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>...</td>\n",
       "      <td>Single Detached</td>\n",
       "      <td>12</td>\n",
       "      <td>M2M</td>\n",
       "      <td>45-49</td>\n",
       "      <td>Male</td>\n",
       "      <td>300000</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>761</td>\n",
       "      <td>FUNDED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>CAX_Train_5</td>\n",
       "      <td>5</td>\n",
       "      <td>350000</td>\n",
       "      <td>3342</td>\n",
       "      <td>29.59</td>\n",
       "      <td>80.00</td>\n",
       "      <td>34.85</td>\n",
       "      <td>360</td>\n",
       "      <td>592000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Single Detached</td>\n",
       "      <td>12</td>\n",
       "      <td>L7G</td>\n",
       "      <td>50-54</td>\n",
       "      <td>Male</td>\n",
       "      <td>147000</td>\n",
       "      <td>8</td>\n",
       "      <td>62</td>\n",
       "      <td>762</td>\n",
       "      <td>FUNDED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unique_ID  MORTGAGE NUMBER  PROPERTY VALUE  MORTGAGE PAYMENT    GDS  \\\n",
       "0  CAX_Train_1                1          900000              5429  61.98   \n",
       "1  CAX_Train_2                2          386000              2179  35.22   \n",
       "2  CAX_Train_3                3          531000              2152  30.97   \n",
       "3  CAX_Train_4                4         1200000              5410  19.04   \n",
       "4  CAX_Train_5                5          350000              3342  29.59   \n",
       "\n",
       "     LTV    TDS  AMORTIZATION  MORTGAGE AMOUNT  RATE  ...    PROPERTY TYPE  \\\n",
       "0  65.00  71.63           360        1040000.0   4.0  ...  Single Detached   \n",
       "1  74.29  40.65           360         390000.0   4.5  ...  Single Detached   \n",
       "2  80.00  35.41           360         424800.0   3.5  ...  Single Detached   \n",
       "3  75.00  34.14           360         960000.0   5.5  ...  Single Detached   \n",
       "4  80.00  34.85           360         592000.0   5.0  ...  Single Detached   \n",
       "\n",
       "  TERM  FSA    AGE RANGE  GENDER  INCOME INCOME TYPE  NAICS CODE  \\\n",
       "0   12  L4C     Under 25    Male  108000           8       44-45   \n",
       "1   12  L9T  70 and over    Male   78000           2          56   \n",
       "2    6  M1N        35-39  Female   87000           2          72   \n",
       "3   12  M2M        45-49    Male  300000           8          54   \n",
       "4   12  L7G        50-54    Male  147000           8          62   \n",
       "\n",
       "   CREDIT SCORE  RESULT  \n",
       "0           681  FUNDED  \n",
       "1           710  FUNDED  \n",
       "2           709  FUNDED  \n",
       "3           761  FUNDED  \n",
       "4           762  FUNDED  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PROPERTY VALUE', 'MORTGAGE PAYMENT', 'GDS', 'LTV', 'TDS',\n",
       "       'AMORTIZATION', 'MORTGAGE AMOUNT', 'RATE', 'MORTGAGE PURPOSE',\n",
       "       'PAYMENT FREQUENCY', 'PROPERTY TYPE', 'TERM', 'FSA', 'AGE RANGE',\n",
       "       'GENDER', 'INCOME', 'INCOME TYPE', 'NAICS CODE', 'CREDIT SCORE',\n",
       "       'RESULT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop irrelevant variables\n",
    "cols_to_drop = ['Unique_ID', 'MORTGAGE NUMBER']\n",
    "train.drop(columns=cols_to_drop, inplace=True)\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_distinct_numeric(df):\n",
    "    variable_name = []\n",
    "    distinct_values = []\n",
    "    for col in df.columns:\n",
    "        variable_name.append(col)\n",
    "        distinct_values.append(len(df[col].unique()))\n",
    "        \n",
    "    df_distinct = pd.DataFrame({'Variable':variable_name, 'Number_of_distinct':distinct_values})\n",
    "    \n",
    "    return df_distinct\n",
    "\n",
    "df_dist = check_distinct_numeric(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Number_of_distinct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>PROPERTY VALUE</td>\n",
       "      <td>1705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>MORTGAGE PAYMENT</td>\n",
       "      <td>6430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>GDS</td>\n",
       "      <td>6027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>LTV</td>\n",
       "      <td>5811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>TDS</td>\n",
       "      <td>5704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Variable  Number_of_distinct\n",
       "0    PROPERTY VALUE                1705\n",
       "1  MORTGAGE PAYMENT                6430\n",
       "2               GDS                6027\n",
       "3               LTV                5811\n",
       "4               TDS                5704"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=train.RESULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_missing = train.isna()\n",
    "train_num_missing = train_missing.sum()\n",
    "train_num_missing / len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = pd.DataFrame(train.dtypes, columns=['type'])\n",
    "data_types['Feature'] = data_types.index\n",
    "data_types.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['Unique_ID', 'RESULT']\n",
    "df_funded = train[train.RESULT == 'FUNDED']\n",
    "df_not_funded = train[train.RESULT == 'NOT FUNDED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(5,5))\n",
    "def plot_hist_object(funded, not_funded, col):\n",
    "    legend = ['Funded', 'Not Funded']\n",
    "    plt.hist([funded, not_funded], color=['red', 'blue'])\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend(legend)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_hist(funded, not_funded, col):\n",
    "    legend = ['Funded', 'Not Funded']\n",
    "    plt.hist([funded, not_funded], color=['red', 'blue'])\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend(legend)\n",
    "    plt.xticks(range(0, 7))\n",
    "    plt.yticks(range(1, 20))\n",
    "    plt.show()z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA on categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop irrelevant variables\n",
    "cols_to_drop = ['Unique_ID','MORTGAGE NUMBER','RESULT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = train.select_dtypes(include=['object', 'bool']).columns\n",
    "cat_col = [col for col in cat_col if col not in ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in data_types.iterrows():\n",
    "    if row.Feature not in cols_to_drop:\n",
    "        col = row.Feature\n",
    "        funded = df_funded[col]\n",
    "        not_funded = df_not_funded[col]\n",
    "        if row.type == \"object\":\n",
    "            plot_hist_object(funded, not_funded, col)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WOE and IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas.core.algorithms as algos\n",
    "from pandas import Series\n",
    "import scipy.stats.stats as stats\n",
    "import re\n",
    "import traceback\n",
    "import string\n",
    "\n",
    "max_bin = 20\n",
    "force_bin = 3\n",
    "\n",
    "# define a binning function\n",
    "def mono_bin(Y, X, n = max_bin):\n",
    "    \n",
    "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
    "    notmiss = df1[['X','Y']][df1.X.notnull()]\n",
    "    r = 0\n",
    "    while np.abs(r) < 1:\n",
    "        try:\n",
    "            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n",
    "            d2 = d1.groupby('Bucket', as_index=True)\n",
    "            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
    "            n = n - 1 \n",
    "        except Exception as e:\n",
    "            n = n - 1\n",
    "\n",
    "    if len(d2) == 1:\n",
    "        n = force_bin         \n",
    "        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n",
    "        if len(np.unique(bins)) == 2:\n",
    "            bins = np.insert(bins, 0, 1)\n",
    "            bins[1] = bins[1]-(bins[1]/2)\n",
    "        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n",
    "        d2 = d1.groupby('Bucket', as_index=True)\n",
    "    \n",
    "    d3 = pd.DataFrame({},index=[])\n",
    "    d3[\"MIN_VALUE\"] = d2.min().X\n",
    "    d3[\"MAX_VALUE\"] = d2.max().X\n",
    "    d3[\"COUNT\"] = d2.count().Y\n",
    "    d3[\"EVENT\"] = d2.sum().Y\n",
    "    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n",
    "    d3=d3.reset_index(drop=True)\n",
    "    \n",
    "    if len(justmiss.index) > 0:\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        d4[\"MAX_VALUE\"] = np.nan\n",
    "        d4[\"COUNT\"] = justmiss.count().Y\n",
    "        d4[\"EVENT\"] = justmiss.sum().Y\n",
    "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
    "        d3 = d3.append(d4,ignore_index=True)\n",
    "    \n",
    "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"VAR_NAME\"] = \"VAR\"\n",
    "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n",
    "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "    d3.IV = d3.IV.sum()\n",
    "    \n",
    "    return(d3)\n",
    "\n",
    "def char_bin(Y, X):\n",
    "        \n",
    "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
    "    notmiss = df1[['X','Y']][df1.X.notnull()]    \n",
    "    df2 = notmiss.groupby('X',as_index=True)\n",
    "    \n",
    "    d3 = pd.DataFrame({},index=[])\n",
    "    d3[\"COUNT\"] = df2.count().Y\n",
    "    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n",
    "    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
    "    d3[\"EVENT\"] = df2.sum().Y\n",
    "    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n",
    "    \n",
    "    if len(justmiss.index) > 0:\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        d4[\"MAX_VALUE\"] = np.nan\n",
    "        d4[\"COUNT\"] = justmiss.count().Y\n",
    "        d4[\"EVENT\"] = justmiss.sum().Y\n",
    "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
    "        d3 = d3.append(d4,ignore_index=True)\n",
    "    \n",
    "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"VAR_NAME\"] = \"VAR\"\n",
    "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n",
    "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "    d3.IV = d3.IV.sum()\n",
    "    d3 = d3.reset_index(drop=True)\n",
    "    \n",
    "    return(d3)\n",
    "\n",
    "def data_vars(df1, target):\n",
    "    \n",
    "    stack = traceback.extract_stack()\n",
    "    filename, lineno, function_name, code = stack[-2]\n",
    "    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n",
    "    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n",
    "    \n",
    "    x = df1.dtypes.index\n",
    "    count = -1\n",
    "    \n",
    "    for i in x:\n",
    "        if i.upper() not in (final.upper()):\n",
    "            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n",
    "                conv = mono_bin(target, df1[i])\n",
    "                conv[\"VAR_NAME\"] = i\n",
    "                count = count + 1\n",
    "            else:\n",
    "                conv = char_bin(target, df1[i])\n",
    "                conv[\"VAR_NAME\"] = i            \n",
    "                count = count + 1\n",
    "                \n",
    "            if count == 0:\n",
    "                iv_df = conv\n",
    "            else:\n",
    "                iv_df = iv_df.append(conv,ignore_index=True)\n",
    "    \n",
    "    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n",
    "    iv = iv.reset_index()\n",
    "    return(iv_df,iv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train\n",
    "df['target'] = df['RESULT'].apply(lambda x : 1 if x == 'FUNDED' else 0)  # Convert to numeric\n",
    "df = df.drop(['Unique_ID', 'RESULT', 'MORTGAGE NUMBER'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = train['target']\n",
    "X = df.drop(['target'],axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.10, random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_iv, IV = data_vars(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IV.sort_values('IV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IV.to_csv('IV.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply WOE values to your dataframe columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_vars_list = x_train.columns\n",
    "transform_prefix = 'new_' # leave this value blank if you need replace the original column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_vars_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_WOE(df, transform_vars_list, final_iv):\n",
    "    \n",
    "    for var in transform_vars_list:\n",
    "        small_df = final_iv[final_iv['VAR_NAME'] == var]\n",
    "        transform_dict = dict(zip(small_df.MAX_VALUE,small_df.WOE))\n",
    "        replace_cmd = ''\n",
    "        replace_cmd1 = ''\n",
    "        for i in sorted(transform_dict.items()):\n",
    "            replace_cmd = replace_cmd + str(i[1]) + str(' if x <= ') + str(i[0]) + ' else '\n",
    "            replace_cmd1 = replace_cmd1 + str(i[1]) + str(' if x == \"') + str(i[0]) + '\" else '\n",
    "        replace_cmd = replace_cmd + '0'\n",
    "        replace_cmd1 = replace_cmd1 + '0'\n",
    "        if replace_cmd != '0':\n",
    "            try:\n",
    "                df[transform_prefix + var] = df[var].apply(lambda x: eval(replace_cmd))\n",
    "            except:\n",
    "                df[transform_prefix + var] = df[var].apply(lambda x: eval(replace_cmd1))\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_new = apply_WOE(x_train, transform_vars_list, final_iv)\n",
    "#x_test_new = apply_WOE(x_test, transform_vars_list, final_iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in transform_vars_list:\n",
    "    small_df = final_iv[final_iv['VAR_NAME'] == var]\n",
    "    transform_dict = dict(zip(small_df.MAX_VALUE,small_df.WOE))\n",
    "    replace_cmd = ''\n",
    "    replace_cmd1 = ''\n",
    "    for i in sorted(transform_dict.items()):\n",
    "        replace_cmd = replace_cmd + str(i[1]) + str(' if x <= ') + str(i[0]) + ' else '\n",
    "        replace_cmd1 = replace_cmd1 + str(i[1]) + str(' if x == \"') + str(i[0]) + '\" else '\n",
    "    replace_cmd = replace_cmd + '0'\n",
    "    replace_cmd1 = replace_cmd1 + '0'\n",
    "    if replace_cmd != '0':\n",
    "        try:\n",
    "            x_train[transform_prefix + var] = x_train[var].apply(lambda x: eval(replace_cmd))\n",
    "        except:\n",
    "            x_train[transform_prefix + var] = x_train[var].apply(lambda x: eval(replace_cmd1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in transform_vars_list:\n",
    "    small_df = final_iv[final_iv['VAR_NAME'] == var]\n",
    "    transform_dict = dict(zip(small_df.MAX_VALUE,small_df.WOE))\n",
    "    replace_cmd = ''\n",
    "    replace_cmd1 = ''\n",
    "    for i in sorted(transform_dict.items()):\n",
    "        replace_cmd = replace_cmd + str(i[1]) + str(' if x <= ') + str(i[0]) + ' else '\n",
    "        replace_cmd1 = replace_cmd1 + str(i[1]) + str(' if x == \"') + str(i[0]) + '\" else '\n",
    "    replace_cmd = replace_cmd + '0'\n",
    "    replace_cmd1 = replace_cmd1 + '0'\n",
    "    if replace_cmd != '0':\n",
    "        try:\n",
    "            x_test[transform_prefix + var] = x_test[var].apply(lambda x: eval(replace_cmd))\n",
    "        except:\n",
    "            x_test[transform_prefix + var] = x_test[var].apply(lambda x: eval(replace_cmd1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import  metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, ytrain, dtest, ytest, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=ytrain.values)\n",
    "        xgtest = xgb.DMatrix(dtest[predictors].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], ytrain, eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(ytrain.values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(ytrain, dtrain_predprob))\n",
    "    \n",
    "    #Predict on testing data:\n",
    "    predprob = alg.predict_proba(dtest[predictors])[:,1]\n",
    "    print('AUC Score (Test): %f' % metrics.roc_auc_score(ytest, predprob))\n",
    "                \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1- Find the number of estimators for a high learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [x for x in train.columns if \"new\" in x]\n",
    "\n",
    "xgb1 = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27)\n",
    "\n",
    "modelfit(xgb1, x_train, y_train, x_test, y_test, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid seach on subsample and max_features\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test1 = {\n",
    "    'max_depth':range(3,10,2),\n",
    "    'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    "                                        min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    "                       param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid seach on subsample and max_features\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test2 = {\n",
    "    'max_depth':[4,5,6],\n",
    "    'min_child_weight':[4,5,6]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,\n",
    "                                        min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch2.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid seach on subsample and max_features\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test2b = {\n",
    "    'min_child_weight':[6,8,10,12]\n",
    "}\n",
    "gsearch2b = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=4,\n",
    "                                        min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test2b, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch2b.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch2b.grid_scores_, gsearch2b.best_params_, gsearch2b.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid seach on subsample and max_features\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test3 = {\n",
    "    'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=4,\n",
    "                                        min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch3.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2 = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=4,\n",
    "        min_child_weight=6,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27)\n",
    "modelfit(xgb2, x_train, y_train, x_test, y_test, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Grid seach on subsample and max_features\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test4 = {\n",
    "    'subsample':[i/10.0 for i in range(6,10)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n",
    "                                        min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch4.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tune subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid seach on subsample and max_features\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test5 = {\n",
    "    'subsample':[i/100.0 for i in range(75,90,5)],\n",
    "    'colsample_bytree':[i/100.0 for i in range(75,90,5)]\n",
    "}\n",
    "gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n",
    "                                        min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch5.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid seach on subsample and max_features\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test6 = {\n",
    "    'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch6 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n",
    "                                        min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test6, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch6.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch6.grid_scores_, gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid seach on subsample and max_features\n",
    "#Choose all predictors except target & IDcols\n",
    "param_test7 = {\n",
    "    'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n",
    "}\n",
    "gsearch7 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4,\n",
    "                                        min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8,\n",
    "                                        objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    "                       param_grid = param_test7, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch7.fit(train[predictors],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsearch7.grid_scores_, gsearch7.best_params_, gsearch7.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb3 = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=4,\n",
    "        min_child_weight=6,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.005,\n",
    "        objective= 'binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27)\n",
    "modelfit(xgb3, x_train, y_train, x_test, y_test, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb4 = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=5000,\n",
    "        max_depth=4,\n",
    "        min_child_weight=6,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.005,\n",
    "        objective= 'binary:logistic',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27)\n",
    "modelfit(xgb4, x_train, y_train, x_test, y_test, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
